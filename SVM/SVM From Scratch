import numpy as np
import matplotlib.pyplot as plt
np.random.seed(57)      

def getFeatureDim(dataset):
    mx = 0
    linesplit1 = dataset.split('\n')
    for line2 in linesplit1:
        spacesplit2 = line2.split()
        if spacesplit2 != []:
            for i2 in range(1,len(spacesplit2)):
                colonsplit2 = spacesplit2[i2].split(':')
                ind = int(colonsplit2[0])                
                if ind >= mx:
                    mx = ind
    return mx

def libsvm2matrix(dataset, featuresize):    
    newdataset = []
    nwlinesplit = dataset.split('\n')    
    for line1 in nwlinesplit:
        spacesplit = line1.split()        
        nwline1 = [0]*(featuresize+1)
        if spacesplit != []:
            nwline1[0] = spacesplit[0]
            for i1 in range(1,len(spacesplit)):
                colonsplit = spacesplit[i1].split(':')
                ind = int(colonsplit[0])                
                nwline1[ind] = float(colonsplit[-1])        
            newdataset.append(nwline1)
    return newdataset

def crossvalidation(k, hyperparamEta, featuresize, epoch, hyperparamMu):

    fil = open('data-splits/data.train')
    filetrain = fil.read()
    data1 = libsvm2matrix(filetrain, featuresize)    
    l = len(data1)/k
    print l
    data = []        
    for i in range(k):
        data.append(data1[i*l:(i+1)*l])
    data = np.array(data)
    data = data.astype(np.float)        
    maxaccuracy = 0
    out = []
    for mu in hyperparamMu:
        print "Cross Validation for mu = ",mu
        for eta in hyperparamEta:
            #print "Cross Validation for eta = ",eta
            valaccuracy = []
            for i3 in range(k):
                                
                test = data[i]
                mask = np.ones(len(data), dtype = bool)
                mask[i] = False
                train = np.concatenate(data[mask])                                                
                initialweight = [0.001]* featuresize
                bias = 0.001
                mxepacc = 0
                for ep in range(epoch):
                    np.random.shuffle(train)
                    neweta = float(eta)/float(eta+ep)
                    weight = perceptron(train,initialweight, bias, neweta, mu)
                    initialweight = weight[1:]                    
                    acc = accuracyCalc(test, weight, featuresize)                    
                    if acc >= mxepacc:
                        mxepacc = acc
                        optwt = weight                
                acc = accuracyCalc(test, optwt, featuresize)                                
                valaccuracy.append(acc)
            avgacc = float(sum(valaccuracy))/float(len(valaccuracy))
            accuracystd = np.std(valaccuracy)
            print "For eta : {}, Average Accuracy = {}".format(eta, avgacc)            
            if avgacc >= maxaccuracy:
                maxaccuracy = avgacc
                out = []
                out.append(eta)
                out.append(mu)
        print '\n'
    print "(a) The best Hyperparameter: Eta = {}, Mu = {}".format(out[0],out[-1])
    print "(b) The cross validation accuracy for best hyperparameter: ", maxaccuracy
    return out            


def svm(dataset, weight, bias, eta):
    lr = 0.0001
    ct = 0
    #weight = weight.astype(np.float)
    for line2 in dataset:                
        ct += 1
        lr = (lr)/(1+ct)
        y = int(line2[0])
        if y == 0:
            y = -1
        factor = y * (np.inner(line2[1:],weight)+bias)        
        if factor <= 1:            
            x = np.asarray(line2[1:])
            #weight = (1 - lr) * weight + (lr * y) * x
            #bias = (1 - lr) * bias + (lr * y) 
            weight =  weight + (lr* 0.5 * y) * x  
            bias = bias + (lr* 0.5 * y) 
        else:
            #weight = (1 - lr) * weight
            weight = weight
    bias = np.asarray(bias)
    return np.append(bias,weight)
'''
def updatecount(dataset, weight, bias, eta, mu):
    update = 0
    for line2 in dataset:        
        y = int(line2[0])        
        factor = y * (np.inner(line2[1:],weight)+bias)        
        if factor < mu:
            update += 1
            x = np.asarray(line2[1:])
            weight = weight + (eta * y) * x
            bias = bias + (eta * y)            
    bias = np.asarray(bias)    
    return update
'''

def accuracyCalc(dataset,weight, featuresize):
    correctcount = 0
    total = 0                   
    bias = np.array(weight[0])
    weight = np.array(weight[1:])    
    for line3 in dataset:
        total += 1        
        y = line3[0]
        x = np.array(line3[1:])
        y1 = np.inner(weight,x)        
        y1 = y1 + bias    
        if y1 < 0:
            y1 = 0
        else:
            y1 = 1
        if y1 == y:            
            correctcount += 1
    accuracy = 100 * (float(correctcount)/float(total))    
    return accuracy            
    


#Driving Code
ftrain = open('data-splits/data.train')
dataset = ftrain.read()
featuresize = getFeatureDim(dataset)
print "featuresize:", featuresize
t = libsvm2matrix(dataset,featuresize)
l = len(t)/10
train = t[:9*l]
dev = np.array(t[9*l:])
dev = dev.astype(np.float)
initialweight = [0.001]* featuresize
bias = 0.001
hyperparamEta = [1, 0.1,0.01]
hyperparamMu = [1, 0.1,0.01]
epoch = 10
k = 5
#etamu = crossvalidation(k, hyperparamEta, featuresize, epoch, hyperparamMu)
#eta = etamu[0]
#mu = etamu[-1]


#Checking on Dev data
mxdevaccr = 0
devaccuracy = []
#updt = []
for ep in range(100):    
    np.random.shuffle(train)
    #neweta = float(eta)/float(1+ep)
    weight = svm(train,initialweight, bias, eta)
    #u = updatecount(t,initialweight, bias, eta, mu)
    #updt.append(u)
    initialweight = weight[1:]
    #devdata = libsvm2matrix(devdata, featuresize)
    accuracy = accuracyCalc(dev, weight, featuresize)
    devaccuracy.append(accuracy)
    if accuracy > mxdevaccr:
        mxdevaccr = accuracy
        optweight = weight
        epo = ep+1
    print "Accuracy:", accuracy
#updatesum = sum(updt[:epo])
#avgupdate = round(float(sum(updt[:epo]))/float(epo),3)        
#print "(c) Total updates on training dataset from Epoch #1 to Best Epoch({}) is {}".format(epo,updatesum)
#print "     And average update on every epoch till the best epoch is {}".format(avgupdate)
print " Best Dev Set accuracy:{} (for Epoch #{})".format(mxdevaccr, epo)
print "     Dev Set accuracy for last Epoch {}".format(accuracy)

#Test on Test data using optimal weight from Dev set
#accuracy = accuracyCalc(testdata, optweight, featuresize)
#print "(e) Test Accuracy:", accuracy


'''
#Plot: Accuracy Vs Epochs
plt.plot(range(1,21),devaccuracy)
plt.ylabel('Development Set Accuracy')
plt.xlabel('Epochs')
plt.show()
'''
ftest = open('data-splits/data.test')
dataset = ftest.read()
featuresize = getFeatureDim(dataset)
t = libsvm2matrix(dataset,featuresize)
test = np.array(t)
test = test.astype(np.float)
accuracy = accuracyCalc(test, optweight, featuresize)
print "Test Accuracy:", accuracy

'''
outfile = 'predicted.csv'
outopen = open(outfile, 'w')

idfile = 'ID.csv'
idopen = open(idfile, 'w')

fevalid = open('data-splits/data.eval.id')
evalid = fevalid.read()
evalid = evalid.split('\n')

for ids in evalid:
    ids = str(ids)
    idopen.write(ids)
    idopen.write('\n')
    

feval = open('data-splits/data.eval.anon')
dataset = feval.read()
featuresize = getFeatureDim(dataset)
t = libsvm2matrix(dataset,featuresize)
evl = np.array(t)
evl = evl.astype(np.float)
for item in evl:
    yval = np.inner(item, optweight)
    if yval < 0:
        y = 0
    else:
        y = 1
    y = str(y)
    outopen.write(y)
    outopen.write('\n')
    
outopen.close()
idopen.close()
'''
